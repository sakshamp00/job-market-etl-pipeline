
# Job Market ETL Pipeline

An end-to-end Python ETL (Extract, Transform, Load) pipeline that collects job market data from a public API, cleans and deduplicates the data, and loads it into a local SQLite database for analysis.

It includes:
- Extraction of job listings from an external API (RapidAPI)
- Data cleaning, deduplication, and timestamping using Pandas
- Loading into a relational SQLite database with primary key constraints
- Modular pipeline orchestration for easy re-runs and updates

This project demonstrates core data engineering concepts such as API ingestion, data transformation with Pandas, idempotent database loading, and pipeline orchestration.

---

# ðŸ“’ Table of Contents

- [Installation](#-installation)
- [Usage](#ï¸-usage)
- [Project Structure](#-project-structure)
- [Example Output](#-example-output)
- [Requirements](#-requirements)
- [Contributing](#-contributing)

---

# ðŸ“¦ Installation

1. Clone the repo:

   ```bash
   git clone https://github.com/sakshamp00/job-market-etl-pipeline.git
   cd etl-pipeline
   ```

2. (Optional) Create and activate a virtual environment:

    ```bash
    python3 -m venv venv
    source venv/bin/activate
    ```

3. Install dependencies:

    ```bash
    pip install -r requirements.txt
    ```

4. Create a ```.env``` file with your API key:

    ```bash
    API_KEY=your_rapidpaikey_here
    ```

---

# ðŸ› ï¸ Usage

Run the main ETL pipeline from the project root:

    python -m job_market_etl_pipeline.main

Youâ€™ll see the console output at each stage of the ETL process:
- Extraction of the raw job data via the API key
- Transformation and cleaning of the data
- Loading the clean data into the SQLite database

---

# ðŸ“ Project Structure

```job-market-etl-pipeline/    # Main package folder
â”œâ”€â”€ etl-pipeline/              # Python module
â”‚   â”œâ”€â”€ __init__.py            # Marks this folder as a Python package
â”‚   â”œâ”€â”€ extract.py             # Extraction of data 
â”‚   â”œâ”€â”€ transform.py           # Transformation of data
â”‚   â”œâ”€â”€ load.py                # Loading data into SQLite datbase
â”‚   â””â”€â”€ main.py                # Entry point for executing the job market ETL pipeline
â”œâ”€â”€ rawData/                   # auto-created at runtime
â”œâ”€â”€ cleanData/                 # auto-created at runtime
â”œâ”€â”€ database/                  # auto-created at runtime
â”œâ”€â”€ .gitignore                 
â”œâ”€â”€ .env                       # API key (keep at root, not inside module)
â”œâ”€â”€ README.md                  # This file
â””â”€â”€ requirements.txt           # list of Python dependencies
```

---

# ðŸ“Š Example Output

After running the pipeline:
```
Starting ETL pipeline...
Extracting job data...
Saved raw data â†’ rawData/jobs_raw_20260130_121508.json
Transforming job data...
Saved clean data â†’ cleanData/jobs_clean_20260130_121508.csv
Loaded data from cleanData/jobs_clean_20260130_121508.csv into the database.
Rows in DB: 19
ETL pipeline finished successfully!
```

Database verification with SQLite:
```sql
SELECT COUNT(*) FROM jobs;
SELECT job_title, employer_name FROM jobs LIMIT 5;
```

---

# ðŸ“œ Requirements

Dependencies are listed in ```requirements.txt```:
```
requests>=2.30.0
pandas>=2.1.0
python-dotenv>=1.0.0
urllib3<2
```

Install them with:
```bash
pip install -r requirements.txt
```

---

# ðŸ¤ Contributing and Future Improvements

Contributions are welcome! Whether itâ€™s improving the documentation, adding features like:

- Incremental loading of new jobs only
- Data visualization dashboards
- Keyword analysis or classification of jobs
- Scheduling automated ETL runs using n8n or Airflow
- Moving raw/clean data to cloud storage (S3, Google Drive)
- Sending notifications or reports after each run

Feel free to open issues or pull requests ðŸŽ‰
